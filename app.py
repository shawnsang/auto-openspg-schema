import streamlit as st
import os
from typing import List, Dict, Any
import json
from datetime import datetime
import time
import zipfile
import shutil

from src.document_processor import DocumentProcessor
from src.schema_generator import SchemaGenerator
from src.schema_manager import SchemaManager
from src.llm_client import LLMClient
from src.logger import logger
from src.chunk_logger import ChunkLogger

def collect_all_chunks_text(source_dirs: List[str]) -> str:
    """æ”¶é›†æ‰€æœ‰ç›®å½•ä¸­çš„åˆ†å—æ–‡æœ¬å¹¶åˆå¹¶ä¸ºä¸€ä¸ªæ–‡ä»¶å†…å®¹
    
    Args:
        source_dirs: åŒ…å«åˆ†å—æ–‡ä»¶çš„ç›®å½•åˆ—è¡¨
        
    Returns:
        str: åˆå¹¶åçš„æ‰€æœ‰åˆ†å—æ–‡æœ¬å†…å®¹
    """
    all_chunks_content = []
    
    for source_dir in source_dirs:
        chunks_dir = os.path.join(source_dir, 'chunks')
        if os.path.exists(chunks_dir):
            # è·å–æ–‡æ¡£åç§°
            doc_name = os.path.basename(source_dir)
            all_chunks_content.append(f"\n{'='*80}\næ–‡æ¡£: {doc_name}\n{'='*80}\n")
            
            # è·å–æ‰€æœ‰chunkæ–‡ä»¶å¹¶æ’åº
            chunk_files = [f for f in os.listdir(chunks_dir) if f.startswith('chunk_') and f.endswith('.txt')]
            chunk_files.sort()
            
            for chunk_file in chunk_files:
                chunk_path = os.path.join(chunks_dir, chunk_file)
                try:
                    with open(chunk_path, 'r', encoding='utf-8') as f:
                        chunk_content = f.read().strip()
                    
                    all_chunks_content.append(f"\n--- {chunk_file} ---\n")
                    all_chunks_content.append(chunk_content)
                    all_chunks_content.append("\n")
                except Exception as e:
                    logger.error(f"è¯»å–åˆ†å—æ–‡ä»¶å¤±è´¥ {chunk_path}: {str(e)}")
                    all_chunks_content.append(f"\n--- {chunk_file} (è¯»å–å¤±è´¥) ---\n")
                    all_chunks_content.append(f"é”™è¯¯: {str(e)}\n")
    
    return ''.join(all_chunks_content)

def collect_all_schemas_text(source_dirs: List[str]) -> str:
    """æ”¶é›†æ‰€æœ‰ç›®å½•ä¸­çš„schemaæ–‡æœ¬å¹¶åˆå¹¶ä¸ºä¸€ä¸ªæ–‡ä»¶å†…å®¹
    
    Args:
        source_dirs: åŒ…å«schemaæ–‡ä»¶çš„ç›®å½•åˆ—è¡¨
        
    Returns:
        str: åˆå¹¶åçš„æ‰€æœ‰schemaæ–‡æœ¬å†…å®¹
    """
    all_schemas_content = []
    
    for source_dir in source_dirs:
        schemas_dir = os.path.join(source_dir, 'schemas')
        if os.path.exists(schemas_dir):
            # è·å–æ–‡æ¡£åç§°
            doc_name = os.path.basename(source_dir)
            all_schemas_content.append(f"\n{'='*80}\næ–‡æ¡£: {doc_name}\n{'='*80}\n")
            
            # è·å–æ‰€æœ‰schemaæ–‡ä»¶å¹¶æ’åº
            schema_files = [f for f in os.listdir(schemas_dir) if f.startswith('schema_') and f.endswith('.txt')]
            schema_files.sort()
            
            for schema_file in schema_files:
                schema_path = os.path.join(schemas_dir, schema_file)
                try:
                    with open(schema_path, 'r', encoding='utf-8') as f:
                        schema_content = f.read().strip()
                    
                    all_schemas_content.append(f"\n--- {schema_file} ---\n")
                    all_schemas_content.append(schema_content)
                    all_schemas_content.append("\n")
                except Exception as e:
                    logger.error(f"è¯»å–schemaæ–‡ä»¶å¤±è´¥ {schema_path}: {str(e)}")
                    all_schemas_content.append(f"\n--- {schema_file} (è¯»å–å¤±è´¥) ---\n")
                    all_schemas_content.append(f"é”™è¯¯: {str(e)}\n")
    
    return ''.join(all_schemas_content)

def create_zip_archive(source_dirs: List[str], zip_filename: str) -> str:
    """åˆ›å»ºåŒ…å«æ ¸å¿ƒæ–‡ä»¶çš„zipå‹ç¼©åŒ…ï¼ŒåªåŒ…å«chunkså’Œschemasæ–‡ä»¶å¤¹å†…å®¹
    
    Args:
        source_dirs: è¦å‹ç¼©çš„ç›®å½•åˆ—è¡¨
        zip_filename: è¾“å‡ºçš„zipæ–‡ä»¶å
        
    Returns:
        str: åˆ›å»ºçš„zipæ–‡ä»¶è·¯å¾„
    """
    try:
        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # åªæ·»åŠ æ ¸å¿ƒæ–‡ä»¶ï¼Œä¸åŒ…å«å¤–å±‚ç›®å½•ç»“æ„
            for i, source_dir in enumerate(source_dirs):
                if os.path.exists(source_dir):
                    doc_name = os.path.basename(source_dir)
                    
                    # æ·»åŠ å¤„ç†æ±‡æ€»æŠ¥å‘Š
                    summary_file = os.path.join(source_dir, 'processing_summary.txt')
                    if os.path.exists(summary_file):
                        zipf.write(summary_file, f'{doc_name}_processing_summary.txt')
                    
                    # æ·»åŠ chunksæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶
                    chunks_dir = os.path.join(source_dir, 'chunks')
                    if os.path.exists(chunks_dir):
                        for file in os.listdir(chunks_dir):
                            if file.endswith('.txt'):
                                file_path = os.path.join(chunks_dir, file)
                                # ä½¿ç”¨æ–‡æ¡£åå‰ç¼€é¿å…æ–‡ä»¶åå†²çª
                                arcname = f'chunks/{doc_name}_{file}'
                                zipf.write(file_path, arcname)
                    
                    # æ·»åŠ schemasæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶
                    schemas_dir = os.path.join(source_dir, 'schemas')
                    if os.path.exists(schemas_dir):
                        for file in os.listdir(schemas_dir):
                            if file.endswith('.txt'):
                                file_path = os.path.join(schemas_dir, file)
                                # ä½¿ç”¨æ–‡æ¡£åå‰ç¼€é¿å…æ–‡ä»¶åå†²çª
                                arcname = f'schemas/{doc_name}_{file}'
                                zipf.write(file_path, arcname)
            
            # æ·»åŠ åˆå¹¶çš„åˆ†å—æ–‡æœ¬æ–‡ä»¶
            all_chunks_text = collect_all_chunks_text(source_dirs)
            if all_chunks_text.strip():
                zipf.writestr('æ‰€æœ‰åˆ†å—æ–‡æœ¬.txt', all_chunks_text)
                logger.info("å·²æ·»åŠ åˆå¹¶çš„åˆ†å—æ–‡æœ¬æ–‡ä»¶åˆ°zipåŒ…")
            
            # æ·»åŠ åˆå¹¶çš„schemaæ–‡æœ¬æ–‡ä»¶
            all_schemas_text = collect_all_schemas_text(source_dirs)
            if all_schemas_text.strip():
                zipf.writestr('æ‰€æœ‰Schemaæ–‡æœ¬.txt', all_schemas_text)
                logger.info("å·²æ·»åŠ åˆå¹¶çš„schemaæ–‡æœ¬æ–‡ä»¶åˆ°zipåŒ…")
                            
        logger.info(f"æˆåŠŸåˆ›å»ºzipæ–‡ä»¶: {zip_filename}")
        return zip_filename
    except Exception as e:
        logger.error(f"åˆ›å»ºzipæ–‡ä»¶å¤±è´¥: {str(e)}")
        raise

def main():
    st.set_page_config(
        page_title="OpenSPG Schema è‡ªåŠ¨ç”Ÿæˆå™¨",
        page_icon="ğŸ”—",
        layout="wide"
    )
    
    st.title("ğŸ”— OpenSPG Schema è‡ªåŠ¨ç”Ÿæˆå™¨")
    st.markdown("---")
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ é…ç½®")
        
        # LLM é…ç½®
        st.subheader("LLM è®¾ç½®")
        
        # ä¸“ä¸šé¢†åŸŸè®¾ç½®
        domain_expertise = st.text_input(
            "ä¸“ä¸šé¢†åŸŸ",
            value="",
            placeholder="ä¾‹å¦‚ï¼šéš§é“é˜²æ’æ°´å·¥ç¨‹ã€å»ºç­‘ç»“æ„è®¾è®¡ã€æœºæ¢°åˆ¶é€ ç­‰",
            help="æŒ‡å®šä¸“ä¸šé¢†åŸŸä»¥æé«˜Schemaçš„ä¸“ä¸šè¯†åˆ«èƒ½åŠ›ï¼Œç•™ç©ºåˆ™ä½¿ç”¨é€šç”¨è®¾ç½®"
        )
        
        # LLM æä¾›å•†é€‰æ‹©
        provider = st.selectbox(
            "LLM æä¾›å•†",
            ["OpenAI", "Ollama"],
            help="é€‰æ‹©è¦ä½¿ç”¨çš„ LLM æœåŠ¡æä¾›å•†"
        )
        
        if provider == "OpenAI":
            api_key = st.text_input("OpenAI API Key", type="password")
            base_url = st.text_input(
                "Base URL (å¯é€‰)", 
                placeholder="https://api.openai.com/v1",
                help="è‡ªå®šä¹‰ OpenAI å…¼å®¹æ¥å£çš„ URL"
            )
            model_name = st.text_input(
                "æ¨¡å‹åç§°",
                value="deepseek-chat",
                help="è¾“å…¥æ¨¡å‹åç§°ï¼Œå¦‚ gpt-4, gpt-3.5-turbo, claude-3-sonnet ç­‰"
            )
        else:  # Ollama
            api_key = None
            base_url = st.text_input(
                "Ollama URL", 
                value="http://localhost:11434",
                help="Ollama æœåŠ¡çš„ URL åœ°å€"
            )
            model_name = st.text_input(
                "æ¨¡å‹åç§°",
                value="llama2",
                help="Ollama ä¸­çš„æ¨¡å‹åç§°ï¼Œå¦‚ llama2, mistral ç­‰"
            )
        
        # æ–‡æ¡£å¤„ç†é…ç½®
        st.subheader("æ–‡æ¡£å¤„ç†è®¾ç½®")
        chunk_size = st.slider("æ–‡æ¡£åˆ†å—å¤§å°", 200, 2000, 500)
        chunk_overlap = st.slider("åˆ†å—é‡å å¤§å°", 0, 100, 0)
        
        # Markdown å¤„ç†é€‰é¡¹
        enable_markdown_semantic = st.checkbox(
            "å¯ç”¨ Markdown è¯­ä¹‰åˆ†å—",
            value=True,
            help="å¯¹ Markdown æ–‡æ¡£è¿›è¡Œè¯­ä¹‰åˆ†å—ï¼Œä¿æŒè¡¨æ ¼å®Œæ•´æ€§ï¼Œæé«˜å®ä½“å…³ç³»æå–è´¨é‡"
        )
        
        # Schema é…ç½®
        st.subheader("Schema è®¾ç½®")
        namespace = st.text_input("å‘½åç©ºé—´", value="Engineering")
        

    
    # åˆå§‹åŒ– session state
    if 'schema_manager' not in st.session_state:
        st.session_state.schema_manager = SchemaManager(namespace)
    
    # åªåœ¨çœŸæ­£éœ€è¦æ—¶åˆå§‹åŒ–processing_resultsï¼Œé¿å…æ„å¤–æ¸…ç©º
    if 'processing_results' not in st.session_state:
        st.session_state.processing_results = []
        logger.debug("åˆå§‹åŒ– processing_results ä¸ºç©ºåˆ—è¡¨")
    else:
        logger.debug(f"processing_results å·²å­˜åœ¨ï¼ŒåŒ…å« {len(st.session_state.processing_results)} ä¸ªç»“æœ")
    
    if 'document_chunks' not in st.session_state:
        st.session_state.document_chunks = []
    
    # æ–‡æ¡£å¤„ç†ç•Œé¢
    show_document_processing_tab(provider, api_key, model_name, base_url, chunk_size, chunk_overlap, namespace, domain_expertise, enable_markdown_semantic)

def show_document_processing_tab(provider, api_key, model_name, base_url, chunk_size, chunk_overlap, namespace, domain_expertise, enable_markdown_semantic):
    """æ˜¾ç¤ºæ–‡æ¡£å¤„ç†tabçš„å†…å®¹"""
    st.header("ğŸ“„ æ–‡æ¡£ä¸Šä¼ ")
    
   
    uploaded_files = st.file_uploader(
        "é€‰æ‹©æ–‡æ¡£æ–‡ä»¶",
        type=["pdf", "docx", "txt", "md", "markdown"],
        accept_multiple_files=True,
        help="æ”¯æŒæ‰¹é‡ä¸Šä¼ ï¼Œå¯åˆ†å¤šæ¬¡å¤„ç†ä¸åŒçš„æ–‡æ¡£é›†åˆã€‚æ”¯æŒæ ¼å¼ï¼šPDFã€Wordæ–‡æ¡£ã€æ–‡æœ¬æ–‡ä»¶ã€Markdownæ–‡æ¡£"
    )
    
    # TODO: æ·»åŠ è·³è¿‡æ–‡æ¡£å¤„ç†çš„é€‰é¡¹ (åŠŸèƒ½æš‚æ—¶ç¦ç”¨)
    # skip_document_processing = st.checkbox(
    #     "ğŸ”§ è·³è¿‡æ–‡æ¡£å¤„ç†ï¼Œç›´æ¥è¿›è¡Œå…³ç³»éªŒè¯",
    #     value=False,
    #     help="å‹¾é€‰æ­¤é€‰é¡¹å°†è·³è¿‡æ–‡æ¡£åˆ†æå’Œå®ä½“æå–ï¼Œç›´æ¥å¯¹å½“å‰Schemaè¿›è¡Œå…³ç³»éªŒè¯å’Œä¼˜åŒ–"
    # )
    skip_document_processing = False  # æš‚æ—¶ç¦ç”¨æ­¤åŠŸèƒ½
    
    if uploaded_files:
        st.success(f"å·²ä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶")
        for file in uploaded_files:
            st.write(f"- {file.name} ({file.size} bytes)")
        
        # æ˜¾ç¤ºå¤„ç†å»ºè®®
        if len(uploaded_files) > 5:
            st.warning(
                f"âš ï¸ å½“å‰ä¸Šä¼ äº† {len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼Œå»ºè®®åˆ†æ‰¹å¤„ç†ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚"
                "æ‚¨å¯ä»¥å…ˆå¤„ç†éƒ¨åˆ†æ–‡ä»¶ï¼Œä¿å­˜ Schema åå†ç»§ç»­å¤„ç†å…¶ä½™æ–‡ä»¶ã€‚"
            )
    
        
    # å¤„ç†æŒ‰é’®
    st.markdown("---")
    
    # æ£€æŸ¥å¿…è¦æ¡ä»¶
    can_process = uploaded_files and (provider == "Ollama" or api_key)
    can_validate_only = skip_document_processing and 'schema_manager' in st.session_state and st.session_state.schema_manager.entities
    
    # å¤„ç†æŒ‰é’®çš„é€»è¾‘
    if skip_document_processing:
        # è·³è¿‡æ–‡æ¡£å¤„ç†æ¨¡å¼
        if st.button("ğŸ”§ æ‰§è¡Œå…³ç³»éªŒè¯", type="primary", disabled=not can_validate_only):
            if not can_validate_only:
                st.error("è¯·å…ˆåŠ è½½æˆ–åˆ›å»ºSchemaï¼Œç„¶åæ‰èƒ½è¿›è¡Œå…³ç³»éªŒè¯")
            else:
                # ç›´æ¥æ‰§è¡Œå…³ç³»éªŒè¯
                with st.spinner("æ­£åœ¨æ‰§è¡Œå…³ç³»éªŒè¯..."):
                    try:
                        validation_result = st.session_state.schema_manager.validate_and_update_relations()
                        
                        # æ£€æŸ¥è¿”å›ç»“æœçš„å®Œæ•´æ€§
                        required_keys = ['updated_entities', 'invalid_relations', 'created_entities', 'merged_relations', 'warnings']
                        missing_keys = [key for key in required_keys if key not in validation_result]
                        if missing_keys:
                            error_msg = f"éªŒè¯ç»“æœç¼ºå°‘å¿…è¦å­—æ®µ: {', '.join(missing_keys)}"
                            logger.error(error_msg)
                            st.error(error_msg)
                        else:
                            # æ˜¾ç¤ºéªŒè¯ç»“æœ
                            display_validation_results(validation_result)
                            st.success("âœ… å…³ç³»éªŒè¯å®Œæˆï¼")
                            
                    except Exception as e:
                        logger.error(f"å…³ç³»éªŒè¯å¤±è´¥: {str(e)}", exc_info=True)
                        st.error(f"âŒ å…³ç³»éªŒè¯å¤±è´¥: {str(e)}")
    else:
        # æ­£å¸¸æ–‡æ¡£å¤„ç†æ¨¡å¼
        if st.button("ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£", type="primary", disabled=not can_process):
            if provider == "OpenAI" and not api_key:
                st.error("è¯·æä¾› OpenAI API Key")
            elif not uploaded_files:
                st.error("è¯·ä¸Šä¼ è‡³å°‘ä¸€ä¸ªæ–‡æ¡£")
            else:
                process_documents(
                    uploaded_files, provider.lower(), api_key, model_name, base_url,
                    chunk_size, chunk_overlap, namespace, domain_expertise, enable_markdown_semantic
                )
    
    # æ˜¾ç¤ºå¤„ç†ç»“æœ
    if st.session_state.processing_results:
        st.header("ğŸ“Š å¤„ç†ç»“æœ")
        
        for i, result in enumerate(st.session_state.processing_results):
            with st.expander(f"æ–‡æ¡£ {i+1}: {result['filename']} - {result['timestamp']}"):
                col_r1, col_r2, col_r3 = st.columns(3)
                
                with col_r1:
                    st.metric("å¤„ç†åˆ†å—", result['stats']['chunks_processed'])
                with col_r2:
                    st.metric("æ–‡ä»¶å¤§å°", f"{result.get('file_size', 'N/A')}")
                with col_r3:
                    st.metric("å¤„ç†æ—¶é—´", result['timestamp'])
    




def process_documents(uploaded_files, provider, api_key, model_name, base_url, chunk_size, chunk_overlap, namespace, domain_expertise="", enable_markdown_semantic=True):
    """å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£"""
    logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£æ‰¹æ¬¡ï¼Œå…± {len(uploaded_files)} ä¸ªæ–‡ä»¶")
    logger.info(f"é…ç½®å‚æ•° - æä¾›å•†: {provider}, æ¨¡å‹: {model_name}, åˆ†å—å¤§å°: {chunk_size}, é‡å : {chunk_overlap}")
    
    # æ¸…ç©ºä¹‹å‰çš„åˆ†å—æ•°æ®
    st.session_state.document_chunks = []
    
    # åˆå§‹åŒ–ç»„ä»¶
    try:
        logger.debug("åˆå§‹åŒ– LLM å®¢æˆ·ç«¯")
        llm_client = LLMClient(
            provider=provider,
            api_key=api_key,
            model_name=model_name,
            base_url=base_url if base_url else None,
            domain_expertise=domain_expertise
        )
        
        # æµ‹è¯•è¿æ¥
        logger.debug(f"æµ‹è¯• {provider} è¿æ¥")
        if not llm_client.test_connection():
            error_msg = f"æ— æ³•è¿æ¥åˆ° {provider} æœåŠ¡ï¼Œè¯·æ£€æŸ¥é…ç½®"
            logger.error(error_msg)
            st.error(error_msg)
            return
        
        success_msg = f"âœ… æˆåŠŸè¿æ¥åˆ° {provider} ({model_name})"
        logger.success(success_msg)
        st.success(success_msg)
        
    except Exception as e:
        error_msg = f"LLM å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        st.error(error_msg)
        return
    
    logger.debug("åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨å’ŒSchemaç”Ÿæˆå™¨")
    doc_processor = DocumentProcessor(
        chunk_size=chunk_size, 
        chunk_overlap=chunk_overlap,
        enable_markdown_semantic=enable_markdown_semantic
    )
    schema_generator = SchemaGenerator(llm_client)
    
    # åˆ›å»ºä¸“ç”¨æ—¥å¿—è®°å½•å™¨
    chunk_logger = ChunkLogger()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs("extracted_entities", exist_ok=True)
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # åˆ›å»ºå®æ—¶æ˜¾ç¤ºå®¹å™¨
    realtime_container = st.container()
    with realtime_container:
        st.subheader("ğŸ”„ å®æ—¶å¤„ç†è¿›åº¦")
        current_file_text = st.empty()
        current_chunk_text = st.empty()
        
        # åˆ†å—å†…å®¹æ˜¾ç¤ºåŒºåŸŸ
        chunk_expander = st.expander("ğŸ“„ å½“å‰åˆ†å—å†…å®¹", expanded=False)
        with chunk_expander:
            chunk_content_area = st.empty()
        
        # LLMå“åº”æ˜¾ç¤ºåŒºåŸŸ
        llm_expander = st.expander("ğŸ¤– LLMå“åº”å†…å®¹", expanded=False)
        with llm_expander:
            llm_response_area = st.empty()
    
    for i, uploaded_file in enumerate(uploaded_files):
        file_progress = (i + 1) / len(uploaded_files)
        logger.info(f"å¤„ç†æ–‡ä»¶ {i+1}/{len(uploaded_files)}: {uploaded_file.name} ({uploaded_file.size} å­—èŠ‚)")
        status_text.text(f"æ­£åœ¨å¤„ç†: {uploaded_file.name}")
        current_file_text.text(f"ğŸ“ å½“å‰æ–‡ä»¶: {uploaded_file.name} ({i + 1}/{len(uploaded_files)})")
        
        try:
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            temp_path = f"temp_{uploaded_file.name}"
            logger.debug(f"ä¿å­˜ä¸´æ—¶æ–‡ä»¶: {temp_path}")
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # å¤„ç†æ–‡æ¡£
            logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {uploaded_file.name}")
            chunks = doc_processor.process_document(temp_path)
            logger.success(f"æ–‡æ¡£å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—")
            
            # ç”Ÿæˆ schema
            logger.info(f"å¼€å§‹ä» {len(chunks)} ä¸ªåˆ†å—ä¸­æå–å®ä½“Schema")
            
            # è®°å½•æ–‡ä»¶å¼€å§‹å¤„ç†
            import time
            file_start_time = time.time()
            
            # ä¸ºå½“å‰æ–‡æ¡£åˆ›å»ºä¸“é—¨çš„ç›®å½•
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            safe_filename = "".join(c for c in uploaded_file.name if c.isalnum() or c in (' ', '-', '_')).rstrip()
            doc_dir = f"extracted_entities/{safe_filename}_{timestamp}"
            os.makedirs(doc_dir, exist_ok=True)
            
            # åˆ›å»ºå­ç›®å½•
            chunks_dir = os.path.join(doc_dir, "chunks")
            schemas_dir = os.path.join(doc_dir, "schemas")
            os.makedirs(chunks_dir, exist_ok=True)
            os.makedirs(schemas_dir, exist_ok=True)
            
            logger.info(f"ä¸ºæ–‡æ¡£ {uploaded_file.name} åˆ›å»ºç›®å½•: {doc_dir}")
            
            for chunk_idx, chunk in enumerate(chunks):
                chunk_start_time = time.time()
                
                logger.debug(f"å¤„ç†åˆ†å— {chunk_idx + 1}/{len(chunks)} ({len(chunk)} å­—ç¬¦)")
                
                # æ›´æ–°çŠ¶æ€æ˜¾ç¤ºå½“å‰å¤„ç†çš„åˆ†å—
                status_text.text(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {uploaded_file.name} çš„åˆ†å— {chunk_idx + 1}/{len(chunks)}...")
                current_chunk_text.text(f"ğŸ“„ å½“å‰åˆ†å—: {chunk_idx + 1}/{len(chunks)} (é•¿åº¦: {len(chunk)} å­—ç¬¦)")
                
                # æ˜¾ç¤ºåˆ†å—å†…å®¹
                try:
                    chunk_preview = chunk[:500] + "..." if len(chunk) > 500 else chunk
                except Exception as e:
                    logger.error(f"åˆ›å»ºchunk_previewæ—¶å‡ºé”™: {str(e)}, chunkç±»å‹: {type(chunk)}")
                    chunk_preview = str(chunk)[:500] + "..." if len(str(chunk)) > 500 else str(chunk)
                chunk_content_area.text_area(
                    f"åˆ†å— {chunk_idx + 1} å†…å®¹é¢„è§ˆ",
                    value=chunk_preview,
                    height=200,
                    disabled=True
                )
                
                # è®°å½•åˆ°ä¸“ç”¨æ—¥å¿—
                chunk_logger.log_chunk_start(uploaded_file.name, chunk_idx, len(chunks))
                chunk_logger.log_chunk_content(chunk, chunk_idx)
                
                # æå–Schemaæ–‡æœ¬
                schema_text = llm_client.extract_entities_from_text(chunk, [])
                logger.debug(f"ä»åˆ†å— {chunk_idx + 1} æå–åˆ°Schemaæ–‡æœ¬é•¿åº¦: {len(schema_text)} å­—ç¬¦")
                
                # æ˜¾ç¤ºLLMå“åº”
                try:
                    llm_preview = schema_text[:500] + "..." if len(schema_text) > 500 else schema_text
                except Exception as e:
                    logger.error(f"åˆ›å»ºllm_previewæ—¶å‡ºé”™: {str(e)}, schema_textç±»å‹: {type(schema_text)}")
                    llm_preview = str(schema_text)[:500] + "..." if len(str(schema_text)) > 500 else str(schema_text)
                llm_response_area.text_area(
                    f"åˆ†å— {chunk_idx + 1} LLMå“åº”é¢„è§ˆ",
                    value=llm_preview,
                    height=200,
                    disabled=True
                )
                
                # è®°å½•LLMå“åº”åˆ°ä¸“ç”¨æ—¥å¿—
                chunk_logger.log_llm_response(schema_text, chunk_idx)
                
                # ä¿å­˜åˆ†å—å†…å®¹åˆ°æ–‡ä»¶
                chunk_filename = f"chunk_{chunk_idx + 1:03d}.txt"
                chunk_filepath = os.path.join(chunks_dir, chunk_filename)
                try:
                    with open(chunk_filepath, 'w', encoding='utf-8') as f:
                        f.write(f"æ–‡ä»¶å: {uploaded_file.name}\n")
                        f.write(f"åˆ†å—åºå·: {chunk_idx + 1}/{len(chunks)}\n")
                        f.write(f"åˆ†å—å¤§å°: {len(chunk)} å­—ç¬¦\n")
                        f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write("=" * 50 + "\n")
                        f.write(str(chunk))  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
                except Exception as e:
                    logger.error(f"ä¿å­˜åˆ†å—æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}, chunkç±»å‹: {type(chunk)}")
                    raise
                
                # ä¿å­˜Schemaå†…å®¹åˆ°æ–‡ä»¶
                schema_filename = f"schema_{chunk_idx + 1:03d}.txt"
                schema_filepath = os.path.join(schemas_dir, schema_filename)
                try:
                    with open(schema_filepath, 'w', encoding='utf-8') as f:
                        f.write(f"æ–‡ä»¶å: {uploaded_file.name}\n")
                        f.write(f"åˆ†å—åºå·: {chunk_idx + 1}/{len(chunks)}\n")
                        f.write(f"Schemaé•¿åº¦: {len(schema_text)} å­—ç¬¦\n")
                        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write("=" * 50 + "\n")
                        f.write(str(schema_text))  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
                except Exception as e:
                    logger.error(f"ä¿å­˜Schemaæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}, schema_textç±»å‹: {type(schema_text)}")
                    raise
                
                logger.debug(f"å·²ä¿å­˜åˆ†å—æ–‡ä»¶: {chunk_filepath}")
                logger.debug(f"å·²ä¿å­˜Schemaæ–‡ä»¶: {schema_filepath}")
                
                chunk_end_time = time.time()
                chunk_processing_time = chunk_end_time - chunk_start_time
                chunk_logger.log_chunk_complete(chunk_idx, chunk_processing_time)
                
                # ä¿å­˜åˆ†å—å’Œå¯¹åº”çš„Schemaæ–‡æœ¬åˆ°session state
                try:
                    chunk_info = {
                        'filename': str(uploaded_file.name),
                        'chunk_index': int(chunk_idx),
                        'total_chunks': int(len(chunks)),
                        'content': str(chunk),
                        'schema_text': str(schema_text),
                        'chunk_file': str(chunk_filepath),
                        'schema_file': str(schema_filepath)
                    }
                    st.session_state.document_chunks.append(chunk_info)
                except Exception as e:
                    logger.error(f"åˆ›å»ºchunk_infoæ—¶å‡ºé”™: {str(e)}")
                    logger.error(f"chunkç±»å‹: {type(chunk)}, schema_textç±»å‹: {type(schema_text)}")
                    raise
                
                # æ›´æ–°è¿›åº¦
                chunk_progress = (chunk_idx + 1) / len(chunks)
                overall_progress = (i + chunk_progress) / len(uploaded_files)
                progress_bar.progress(overall_progress)
            
            # è®°å½•æ–‡ä»¶å¤„ç†å®Œæˆ
            file_end_time = time.time()
            file_processing_time = file_end_time - file_start_time
            chunk_logger.log_file_complete(uploaded_file.name, len(chunks), file_processing_time)
            
            # åˆ›å»ºæ–‡æ¡£å¤„ç†æ±‡æ€»æ–‡ä»¶
            summary_file = os.path.join(doc_dir, "processing_summary.txt")
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(f"æ–‡æ¡£å¤„ç†æ±‡æ€»æŠ¥å‘Š\n")
                f.write(f"{'=' * 50}\n")
                f.write(f"åŸå§‹æ–‡ä»¶å: {uploaded_file.name}\n")
                f.write(f"æ–‡ä»¶å¤§å°: {uploaded_file.size} å­—èŠ‚\n")
                f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"å¤„ç†è€—æ—¶: {file_processing_time:.2f} ç§’\n")
                f.write(f"åˆ†å—æ€»æ•°: {len(chunks)}\n")
                f.write(f"è¾“å‡ºç›®å½•: {doc_dir}\n")
                f.write(f"\nåˆ†å—æ–‡ä»¶åˆ—è¡¨:\n")
                f.write(f"{'-' * 30}\n")
                for idx in range(len(chunks)):
                    f.write(f"åˆ†å— {idx + 1:03d}: chunks/chunk_{idx + 1:03d}.txt\n")
                f.write(f"\nSchemaæ–‡ä»¶åˆ—è¡¨:\n")
                f.write(f"{'-' * 30}\n")
                for idx in range(len(chunks)):
                    f.write(f"Schema {idx + 1:03d}: schemas/schema_{idx + 1:03d}.txt\n")
            
            logger.info(f"å·²åˆ›å»ºå¤„ç†æ±‡æ€»æ–‡ä»¶: {summary_file}")
            
            # è®°å½•å¤„ç†ç»“æœ
            result = {
                'filename': uploaded_file.name,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'output_dir': doc_dir,
                'stats': {
                    'chunks_processed': len(chunks)
                }
            }
            
            st.session_state.processing_results.append(result)
            logger.success(f"æ–‡ä»¶ {uploaded_file.name} å¤„ç†å®Œæˆï¼Œç”Ÿæˆäº† {len(chunks)} ä¸ªåˆ†å—çš„Schemaå®šä¹‰")
            
            # ç¡®ä¿session stateè¢«æ­£ç¡®æ›´æ–°
            logger.debug(f"å½“å‰ processing_results åŒ…å« {len(st.session_state.processing_results)} ä¸ªç»“æœ")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            logger.debug(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_path}")
            os.remove(temp_path)
            
        except Exception as e:
            error_msg = f"å¤„ç†æ–‡æ¡£ {uploaded_file.name} æ—¶å‡ºé”™: {str(e)}"
            logger.error(error_msg, exc_info=True)
            st.error(error_msg)
            
            # æ¸…ç†å¯èƒ½å­˜åœ¨çš„ä¸´æ—¶æ–‡ä»¶
            temp_path = f"temp_{uploaded_file.name}"
            if os.path.exists(temp_path):
                logger.debug(f"æ¸…ç†é”™è¯¯å¤„ç†ä¸­çš„ä¸´æ—¶æ–‡ä»¶: {temp_path}")
                os.remove(temp_path)
        
        # æ›´æ–°è¿›åº¦
        progress_bar.progress((i + 1) / len(uploaded_files))
    
    # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
    total_files = len(uploaded_files)
    successful_files = len([r for r in st.session_state.processing_results if 'stats' in r])
    failed_files = total_files - successful_files

    logger.success(f"æ–‡æ¡£æ‰¹æ¬¡å¤„ç†å®Œæˆ - æ€»è®¡: {total_files}, æˆåŠŸ: {successful_files}, å¤±è´¥: {failed_files}")

    if st.session_state.processing_results:
        total_chunks = sum(r.get('stats', {}).get('chunks_processed', 0) for r in st.session_state.processing_results)
        logger.info(f"åˆ†å—ç»Ÿè®¡ - æ€»åˆ†å—æ•°: {total_chunks}")

    progress_bar.progress(1.0)
    status_text.text("âœ… æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆï¼")
    
    # æ˜¾ç¤ºå¤„ç†å®Œæˆä¿¡æ¯å’Œæ—¥å¿—æ–‡ä»¶ä½ç½®
    st.success(f"æˆåŠŸå¤„ç† {len(uploaded_files)} ä¸ªæ–‡æ¡£")
    
    # æ˜¾ç¤ºæ–‡ä»¶ä¿å­˜ä¿¡æ¯
    if st.session_state.processing_results:
        st.subheader("ğŸ“ æ–‡ä»¶ä¿å­˜ä½ç½®")
        for result in st.session_state.processing_results:
            if 'output_dir' in result:
                st.info(f"ğŸ“„ **{result['filename']}** çš„åˆ†å—å’ŒSchemaæ–‡ä»¶å·²ä¿å­˜åˆ°: `{result['output_dir']}`")
                
                # æ˜¾ç¤ºç›®å½•ç»“æ„
                with st.expander(f"æŸ¥çœ‹ {result['filename']} çš„è¾“å‡ºç›®å½•ç»“æ„", expanded=False):
                    if os.path.exists(result['output_dir']):
                        st.text(f"{result['output_dir']}/")
                        st.text(f"â”œâ”€â”€ processing_summary.txt  (å¤„ç†æ±‡æ€»æŠ¥å‘Š)")
                        st.text(f"â”œâ”€â”€ chunks/                 (åˆ†å—æ–‡ä»¶ç›®å½•)")
                        chunks_count = result.get('stats', {}).get('chunks_processed', 0)
                        for i in range(chunks_count):
                            st.text(f"â”‚   â”œâ”€â”€ chunk_{i+1:03d}.txt")
                        st.text(f"â””â”€â”€ schemas/                (Schemaæ–‡ä»¶ç›®å½•)")
                        for i in range(chunks_count):
                            st.text(f"    â”œâ”€â”€ schema_{i+1:03d}.txt")
    
    # æ˜¾ç¤ºæ—¥å¿—æ–‡ä»¶ä¿¡æ¯
    log_file_path = chunk_logger.get_log_file_path()
    st.info(f"ğŸ“ è¯¦ç»†çš„å¤„ç†æ—¥å¿—å·²è®°å½•åˆ°: `{log_file_path}`")
    
    # æä¾›ä¸‹è½½æ—¥å¿—æ–‡ä»¶çš„é€‰é¡¹
    if os.path.exists(log_file_path):
        with open(log_file_path, 'r', encoding='utf-8') as f:
            log_content = f.read()
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½å¤„ç†æ—¥å¿—",
            data=log_content,
            file_name=f"chunk_processing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
            mime="text/plain"
        )
    
    # åˆ›å»ºå¹¶æä¾›zipæ–‡ä»¶ä¸‹è½½ - å§‹ç»ˆæ˜¾ç¤ºä¸‹è½½åŒºåŸŸ
    st.subheader("ğŸ“¦ æ‰“åŒ…ä¸‹è½½")
    
    if st.session_state.processing_results:
        logger.debug(f"å‡†å¤‡æ˜¾ç¤ºä¸‹è½½åŒºåŸŸï¼Œprocessing_results åŒ…å« {len(st.session_state.processing_results)} ä¸ªç»“æœ")
        
        # æ”¶é›†æ‰€æœ‰è¾“å‡ºç›®å½•
        output_dirs = []
        for result in st.session_state.processing_results:
            if 'output_dir' in result:
                if os.path.exists(result['output_dir']):
                    output_dirs.append(result['output_dir'])
                    logger.debug(f"æ·»åŠ è¾“å‡ºç›®å½•: {result['output_dir']}")
                else:
                    logger.warning(f"è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {result['output_dir']}")
            else:
                logger.warning(f"å¤„ç†ç»“æœç¼ºå°‘ output_dir å­—æ®µ: {result}")
        
        logger.debug(f"æ”¶é›†åˆ° {len(output_dirs)} ä¸ªæœ‰æ•ˆè¾“å‡ºç›®å½•")
        if output_dirs:
            # åˆ›å»ºzipæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            zip_filename = f"extracted_entities_{timestamp}.zip"
            
            try:
                # æ˜¾ç¤ºæ‰“åŒ…è¿›åº¦
                with st.spinner("æ­£åœ¨æ‰“åŒ…æ–‡ä»¶..."):
                    zip_path = create_zip_archive(output_dirs, zip_filename)
                
                # è¯»å–zipæ–‡ä»¶å†…å®¹ç”¨äºä¸‹è½½
                with open(zip_path, 'rb') as f:
                    zip_data = f.read()
                
                # æä¾›å•ç‹¬çš„æ–‡æœ¬æ–‡ä»¶ä¸‹è½½
                st.subheader("ğŸ“„ å•ç‹¬ä¸‹è½½æ–‡æœ¬æ–‡ä»¶")
                
                # ç”Ÿæˆåˆå¹¶çš„åˆ†å—æ–‡æœ¬
                all_chunks_text = collect_all_chunks_text(output_dirs)
                if all_chunks_text.strip():
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½æ‰€æœ‰åˆ†å—æ–‡æœ¬",
                        data=all_chunks_text,
                        file_name=f"æ‰€æœ‰åˆ†å—æ–‡æœ¬_{timestamp}.txt",
                        mime="text/plain",
                        help="åŒ…å«æ‰€æœ‰æ–‡æ¡£çš„åˆ†å—å†…å®¹ï¼ŒæŒ‰æ–‡æ¡£å’Œåˆ†å—é¡ºåºæ’åˆ—"
                    )
                
                # ç”Ÿæˆåˆå¹¶çš„schemaæ–‡æœ¬
                all_schemas_text = collect_all_schemas_text(output_dirs)
                if all_schemas_text.strip():
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½æ‰€æœ‰Schemaæ–‡æœ¬",
                        data=all_schemas_text,
                        file_name=f"æ‰€æœ‰Schemaæ–‡æœ¬_{timestamp}.txt",
                        mime="text/plain",
                        help="åŒ…å«æ‰€æœ‰æ–‡æ¡£çš„Schemaå®šä¹‰ï¼ŒæŒ‰æ–‡æ¡£å’Œåˆ†å—é¡ºåºæ’åˆ—"
                    )
                
                st.markdown("---")
                
                # æä¾›å®Œæ•´å‹ç¼©åŒ…ä¸‹è½½
                st.subheader("ğŸ“¦ å®Œæ•´å‹ç¼©åŒ…ä¸‹è½½")
                st.success(f"âœ… æ‰“åŒ…å®Œæˆï¼æ–‡ä»¶å¤§å°: {len(zip_data) / 1024 / 1024:.2f} MB")
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½å®Œæ•´å‹ç¼©åŒ…",
                    data=zip_data,
                    file_name=zip_filename,
                    mime="application/zip",
                    help="åŒ…å«æ‰€æœ‰æ–‡æ¡£çš„åˆ†å—æ–‡ä»¶ã€Schemaå®šä¹‰ï¼Œä»¥åŠåˆå¹¶çš„åˆ†å—æ–‡æœ¬å’ŒSchemaæ–‡æœ¬"
                )
                
                # æ˜¾ç¤ºzipæ–‡ä»¶å†…å®¹é¢„è§ˆ
                with st.expander("ğŸ“‹ å‹ç¼©åŒ…å†…å®¹é¢„è§ˆ", expanded=False):
                    st.text("å‹ç¼©åŒ…åŒ…å«ä»¥ä¸‹æ–‡ä»¶:")
                    
                    # æ˜¾ç¤ºåˆå¹¶çš„æ–‡æœ¬æ–‡ä»¶
                    st.text("ğŸ“„ æ‰€æœ‰åˆ†å—æ–‡æœ¬.txt  (æ‰€æœ‰æ–‡æ¡£çš„åˆ†å—å†…å®¹åˆå¹¶)")
                    st.text("ğŸ“„ æ‰€æœ‰Schemaæ–‡æœ¬.txt  (æ‰€æœ‰æ–‡æ¡£çš„Schemaå®šä¹‰åˆå¹¶)")
                    st.text("")
                    
                    # æ˜¾ç¤ºå¤„ç†æ±‡æ€»æŠ¥å‘Š
                    for output_dir in output_dirs:
                        doc_name = os.path.basename(output_dir)
                        st.text(f"ğŸ“„ {doc_name}_processing_summary.txt  (å¤„ç†æ±‡æ€»æŠ¥å‘Š)")
                    st.text("")
                    
                    # æ˜¾ç¤ºchunksç›®å½•
                    st.text("ğŸ“ chunks/")
                    for output_dir in output_dirs:
                        doc_name = os.path.basename(output_dir)
                        chunks_dir = os.path.join(output_dir, 'chunks')
                        if os.path.exists(chunks_dir):
                            chunk_files = [f for f in os.listdir(chunks_dir) if f.endswith('.txt')]
                            for chunk_file in sorted(chunk_files):
                                st.text(f"  ğŸ“„ {doc_name}_{chunk_file}")
                    st.text("")
                    
                    # æ˜¾ç¤ºschemasç›®å½•
                    st.text("ğŸ“ schemas/")
                    for output_dir in output_dirs:
                        doc_name = os.path.basename(output_dir)
                        schemas_dir = os.path.join(output_dir, 'schemas')
                        if os.path.exists(schemas_dir):
                            schema_files = [f for f in os.listdir(schemas_dir) if f.endswith('.txt')]
                            for schema_file in sorted(schema_files):
                                st.text(f"  ğŸ“„ {doc_name}_{schema_file}")
                
                # æ¸…ç†ä¸´æ—¶zipæ–‡ä»¶ï¼ˆå¯é€‰ï¼Œä¹Ÿå¯ä»¥ä¿ç•™ä¾›åç»­ä½¿ç”¨ï¼‰
                # os.remove(zip_path)
                
            except Exception as e:
                st.error(f"æ‰“åŒ…æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                logger.error(f"åˆ›å»ºzipæ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
        else:
            st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¾“å‡ºç›®å½•ï¼Œæ— æ³•åˆ›å»ºä¸‹è½½åŒ…ã€‚è¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²æ­£ç¡®å¤„ç†ã€‚")
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„è¾“å‡ºç›®å½•å¯ç”¨äºåˆ›å»ºä¸‹è½½åŒ…")
    else:
        st.info("ğŸ“ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£ï¼Œç„¶åå°±å¯ä»¥åœ¨è¿™é‡Œä¸‹è½½å¤„ç†ç»“æœäº†ã€‚")
        logger.debug("processing_results ä¸ºç©ºï¼Œæ˜¾ç¤ºæç¤ºä¿¡æ¯")

def display_validation_results(validation_result):
    """æ˜¾ç¤ºå…³ç³»éªŒè¯ç»“æœ"""
    if validation_result.get('updated_entities'):
        logger.info(f"æ›´æ–°äº† {len(validation_result['updated_entities'])} ä¸ªå®ä½“çš„relations")
        st.success(f"âœ… æˆåŠŸæ›´æ–°äº† {len(validation_result['updated_entities'])} ä¸ªå®ä½“çš„å…³ç³»å¼•ç”¨")
        
        # æ˜¾ç¤ºæ›´æ–°è¯¦æƒ…
        with st.expander("æŸ¥çœ‹å…³ç³»æ›´æ–°è¯¦æƒ…"):
            for update in validation_result['updated_entities']:
                st.write(f"**{update['entity']}** - {update['relation']}: {update['old_target']} â†’ {update['new_target']}")
    
    if validation_result.get('created_entities'):
        logger.info(f"è‡ªåŠ¨åˆ›å»ºäº† {len(validation_result['created_entities'])} ä¸ªç¼ºå¤±çš„å®ä½“")
        st.success(f"ğŸ†• è‡ªåŠ¨åˆ›å»ºäº† {len(validation_result['created_entities'])} ä¸ªç¼ºå¤±çš„å®ä½“")
        
        # æ˜¾ç¤ºåˆ›å»ºè¯¦æƒ…
        with st.expander("æŸ¥çœ‹è‡ªåŠ¨åˆ›å»ºçš„å®ä½“è¯¦æƒ…"):
            for created in validation_result['created_entities']:
                st.write(f"**{created['entity']}** - {created['reason']}")
    
    if validation_result.get('merged_relations'):
        logger.info(f"åˆå¹¶äº† {len(validation_result['merged_relations'])} ç»„é‡å¤å…³ç³»")
        st.success(f"ğŸ”— åˆå¹¶äº† {len(validation_result['merged_relations'])} ç»„é‡å¤å…³ç³»")
        
        # æ˜¾ç¤ºåˆå¹¶è¯¦æƒ…
        with st.expander("æŸ¥çœ‹å…³ç³»åˆå¹¶è¯¦æƒ…"):
            for merged in validation_result['merged_relations']:
                st.write(f"**{merged['entity']}** â†’ **{merged['target']}**:")
                st.write(f"  ä¸»å…³ç³»: {merged['primary_relation']}")
                st.write(f"  åˆå¹¶çš„å…³ç³»: {', '.join(merged['merged_relations'])}")
                st.write(f"  æ‰€æœ‰åç§°: {', '.join(merged['all_names'])}")
    
    if validation_result.get('invalid_relations'):
        logger.warning(f"å‘ç° {len(validation_result['invalid_relations'])} ä¸ªæ— æ•ˆçš„å…³ç³»å¼•ç”¨")
        st.warning(f"âš ï¸ å‘ç° {len(validation_result['invalid_relations'])} ä¸ªæ— æ•ˆçš„å…³ç³»å¼•ç”¨")
        
        # æ˜¾ç¤ºæ— æ•ˆå…³ç³»è¯¦æƒ…
        with st.expander("æŸ¥çœ‹æ— æ•ˆå…³ç³»è¯¦æƒ…"):
            for invalid in validation_result['invalid_relations']:
                st.write(f"**{invalid['entity']}** - {invalid['relation']}: {invalid['target']} ({invalid['reason']})")
    
    if (not validation_result.get('updated_entities') and 
        not validation_result.get('created_entities') and 
        not validation_result.get('merged_relations') and 
        not validation_result.get('invalid_relations')):
        st.info("âœ… æ‰€æœ‰å…³ç³»å¼•ç”¨éƒ½æ˜¯æœ‰æ•ˆçš„ï¼Œæ— éœ€æ›´æ–°")

if __name__ == "__main__":
    main()